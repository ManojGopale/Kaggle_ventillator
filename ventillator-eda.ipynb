{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350aca16",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-26T04:20:18.197519Z",
     "iopub.status.busy": "2021-10-26T04:20:18.195979Z",
     "iopub.status.idle": "2021-10-26T04:20:18.215597Z",
     "shell.execute_reply": "2021-10-26T04:20:18.215035Z",
     "shell.execute_reply.started": "2021-10-25T18:30:14.463256Z"
    },
    "papermill": {
     "duration": 0.032769,
     "end_time": "2021-10-26T04:20:18.215726",
     "exception": false,
     "start_time": "2021-10-26T04:20:18.182957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ventilator-pressure-prediction/sample_submission.csv\n",
      "/kaggle/input/ventilator-pressure-prediction/train.csv\n",
      "/kaggle/input/ventilator-pressure-prediction/test.csv\n",
      "/kaggle/input/rnn-64-128/ventilator_extraFeatures_biDirectional1_trial.h5\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7356a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T04:20:18.240994Z",
     "iopub.status.busy": "2021-10-26T04:20:18.240228Z",
     "iopub.status.idle": "2021-10-26T04:20:30.760010Z",
     "shell.execute_reply": "2021-10-26T04:20:30.759544Z",
     "shell.execute_reply.started": "2021-10-26T03:55:07.942510Z"
    },
    "papermill": {
     "duration": 12.534234,
     "end_time": "2021-10-26T04:20:30.760143",
     "exception": false,
     "start_time": "2021-10-26T04:20:18.225909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6036000, 8)\n"
     ]
    }
   ],
   "source": [
    "##Analyze the ventillator pressure dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "##Create a NN model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "import gc\n",
    "\n",
    "\n",
    "trainDataPath = \"/kaggle/input/ventilator-pressure-prediction/train.csv\"\n",
    "df_data = pd.read_csv(trainDataPath, header=0)\n",
    "df_data_orig = df_data.copy()\n",
    "df_data.head()\n",
    "#df_data.describe()\n",
    "\n",
    "# testDataPath = \"/kaggle/input/ventilator-pressure-prediction/test.csv\"\n",
    "# df_test = pd.read_csv(testDataPath, header=0)\n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "DEBUG= False\n",
    "if DEBUG:\n",
    "    df_data = df_data.iloc[:80*100, :]\n",
    "    #df_test = df_test.iloc[:80*100, :]\n",
    "print(df_data.shape)\n",
    "#print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa248f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T04:20:30.805021Z",
     "iopub.status.busy": "2021-10-26T04:20:30.804208Z",
     "iopub.status.idle": "2021-10-26T04:20:30.806433Z",
     "shell.execute_reply": "2021-10-26T04:20:30.806910Z",
     "shell.execute_reply.started": "2021-10-26T03:59:18.840174Z"
    },
    "papermill": {
     "duration": 0.03822,
     "end_time": "2021-10-26T04:20:30.807039",
     "exception": false,
     "start_time": "2021-10-26T04:20:30.768819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Preprocess the data for the model\n",
    "def addFeatures(df_data):\n",
    "    df_data[\"u_in_prev1\"] = df_data.groupby(\"breath_id\")[\"u_in\"].shift(1, fill_value=0)\n",
    "    df_data[\"u_in_prev2\"] = df_data.groupby(\"breath_id\")[\"u_in\"].shift(2, fill_value=0)\n",
    "    df_data[\"u_in_prev3\"] = df_data.groupby(\"breath_id\")[\"u_in\"].shift(3, fill_value=0)\n",
    "    df_data[\"u_in_prev4\"] = df_data.groupby(\"breath_id\")[\"u_in\"].shift(4, fill_value=0)\n",
    "    df_data[\"u_in_prev5\"] = df_data.groupby(\"breath_id\")[\"u_in\"].shift(5, fill_value=0)\n",
    "    df_data[\"u_in_prev6\"] = df_data.groupby(\"breath_id\")[\"u_in\"].shift(6, fill_value=0)\n",
    "    df_data[\"u_in_next1\"] = df_data.groupby(\"breath_id\")[\"u_in\"].shift(-1, fill_value=0)\n",
    "    df_data[\"u_in_next2\"] = df_data.groupby(\"breath_id\")[\"u_in\"].shift(-2, fill_value=0)\n",
    "    df_data[\"u_in_next3\"] = df_data.groupby(\"breath_id\")[\"u_in\"].shift(-3, fill_value=0)\n",
    "    df_data[\"u_in_next4\"] = df_data.groupby(\"breath_id\")[\"u_in\"].shift(-4, fill_value=0)\n",
    "    df_data[\"u_in_next5\"] = df_data.groupby(\"breath_id\")[\"u_in\"].shift(-5, fill_value=0)\n",
    "    df_data[\"u_in_next6\"] = df_data.groupby(\"breath_id\")[\"u_in\"].shift(-6, fill_value=0)\n",
    "    df_data[\"u_in_cumm\"] = df_data.groupby(\"breath_id\")[\"u_in\"].cumsum()\n",
    "    df_data[\"area\"] = df_data[\"time_step\"]*df_data[\"u_in\"]\n",
    "    df_data[\"area\"] = df_data.groupby(\"breath_id\")[\"area\"].cumsum()\n",
    "    df_data[\"u_in_diff1\"] = df_data[\"u_in\"] - df_data[\"u_in_prev1\"]\n",
    "    df_data[\"u_in_diff2\"] = df_data[\"u_in\"] - df_data[\"u_in_prev2\"]\n",
    "    df_data[\"u_in_diff3\"] = df_data[\"u_in\"] - df_data[\"u_in_prev3\"]\n",
    "    df_data[\"u_in_diff4\"] = df_data[\"u_in\"] - df_data[\"u_in_prev4\"]\n",
    "    df_data[\"u_in_diff5\"] = df_data[\"u_in\"] - df_data[\"u_in_prev5\"]\n",
    "    df_data[\"u_in_diff6\"] = df_data[\"u_in\"] - df_data[\"u_in_prev6\"]\n",
    "\n",
    "    df_data[\"u_out_prev1\"] = df_data.groupby(\"breath_id\")[\"u_out\"].shift(1, fill_value=0)\n",
    "    df_data[\"u_out_prev2\"] = df_data.groupby(\"breath_id\")[\"u_out\"].shift(2, fill_value=0)\n",
    "    df_data[\"u_out_prev3\"] = df_data.groupby(\"breath_id\")[\"u_out\"].shift(3, fill_value=0)\n",
    "    df_data[\"u_out_prev4\"] = df_data.groupby(\"breath_id\")[\"u_out\"].shift(4, fill_value=0)\n",
    "    df_data[\"u_out_prev5\"] = df_data.groupby(\"breath_id\")[\"u_out\"].shift(5, fill_value=0)\n",
    "    df_data[\"u_out_prev6\"] = df_data.groupby(\"breath_id\")[\"u_out\"].shift(6, fill_value=0)\n",
    "    df_data[\"u_out_next1\"] = df_data.groupby(\"breath_id\")[\"u_out\"].shift(-1, fill_value=0)\n",
    "    df_data[\"u_out_next2\"] = df_data.groupby(\"breath_id\")[\"u_out\"].shift(-2, fill_value=0)\n",
    "    df_data[\"u_out_next3\"] = df_data.groupby(\"breath_id\")[\"u_out\"].shift(-3, fill_value=0)\n",
    "    df_data[\"u_out_next4\"] = df_data.groupby(\"breath_id\")[\"u_out\"].shift(-4, fill_value=0)\n",
    "    df_data[\"u_out_next5\"] = df_data.groupby(\"breath_id\")[\"u_out\"].shift(-5, fill_value=0)\n",
    "    df_data[\"u_out_next6\"] = df_data.groupby(\"breath_id\")[\"u_out\"].shift(-6, fill_value=0)\n",
    "    df_data[\"u_out_diff1\"] = df_data[\"u_out\"] - df_data[\"u_out_prev1\"]\n",
    "    df_data[\"u_out_diff2\"] = df_data[\"u_out\"] - df_data[\"u_out_prev2\"]\n",
    "    df_data[\"u_out_diff3\"] = df_data[\"u_out\"] - df_data[\"u_out_prev3\"]\n",
    "    df_data[\"u_out_diff4\"] = df_data[\"u_out\"] - df_data[\"u_out_prev4\"]\n",
    "    df_data[\"u_out_diff5\"] = df_data[\"u_out\"] - df_data[\"u_out_prev5\"]\n",
    "    df_data[\"u_out_diff6\"] = df_data[\"u_out\"] - df_data[\"u_out_prev6\"]\n",
    "\n",
    "    df_data[\"u_in_diff_max\"] = df_data.groupby(\"breath_id\")[\"u_in\"].transform(\"max\") - df_data[\"u_in\"]\n",
    "    df_data[\"u_in_diff_mean\"] = df_data.groupby(\"breath_id\")[\"u_in\"].transform(\"mean\") - df_data[\"u_in\"]\n",
    "\n",
    "    df_data[\"u_in_u_out\"] = df_data[\"u_in\"] * df_data[\"u_out\"]\n",
    "    df_data[\"u_out_timestep\"] = df_data[\"u_out\"] * df_data[\"time_step\"]\n",
    "    \n",
    "    df_data['R_div_C'] = df_data[\"R\"].div(df_data[\"C\"])\n",
    "    df_data['R'] = df_data['R'].astype(str)\n",
    "    df_data['C'] = df_data['C'].astype(str)\n",
    "    df_data['R__C'] = df_data[\"R\"].astype(str) + '__' + df_data[\"C\"].astype(str)\n",
    "    df_data = pd.get_dummies(df_data)\n",
    "    \n",
    "    df_data['time_step_cumsum'] = df_data.groupby(['breath_id'])['time_step'].cumsum()\n",
    "    df_data[\"ewm_u_in_mean\"] = df_data.groupby('breath_id')['u_in'].ewm(halflife=9)\\\n",
    "    .mean().reset_index(level=0,drop=True)\n",
    "    df_data[\"ewm_u_in_std\"] = df_data.groupby('breath_id')['u_in'].ewm(halflife=10)\\\n",
    "    .std().reset_index(level=0,drop=True)\n",
    "    df_data[\"ewm_u_in_corr\"] = df_data.groupby('breath_id')['u_in'].ewm(halflife=15)\\\n",
    "    .corr().reset_index(level=0,drop=True)\n",
    "    df_data[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\",\"15_out_std\"]]=df_data.groupby('breath_id')['u_in']\\\n",
    "    .rolling(window=15,min_periods=1)\\\n",
    "    .agg({\"15_in_sum\":\"sum\",\"15_in_min\":\"min\",\"15_in_max\":\"max\",\"15_in_mean\":\"mean\",\"15_in_std\":\"std\"})\\\n",
    "    .reset_index(level=0,drop=True)\n",
    "    df_data.fillna(0, inplace=True)\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5175074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T04:20:30.836110Z",
     "iopub.status.busy": "2021-10-26T04:20:30.834731Z",
     "iopub.status.idle": "2021-10-26T04:20:30.836915Z",
     "shell.execute_reply": "2021-10-26T04:20:30.837317Z",
     "shell.execute_reply.started": "2021-10-26T03:59:22.566114Z"
    },
    "papermill": {
     "duration": 0.021733,
     "end_time": "2021-10-26T04:20:30.837432",
     "exception": false,
     "start_time": "2021-10-26T04:20:30.815699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased from {:5.2f} Mb to {:5.2f} Mb ({:.1f}% reduction)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42037ff4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T04:20:30.859828Z",
     "iopub.status.busy": "2021-10-26T04:20:30.859066Z",
     "iopub.status.idle": "2021-10-26T04:20:30.861062Z",
     "shell.execute_reply": "2021-10-26T04:20:30.861437Z",
     "shell.execute_reply.started": "2021-10-26T03:59:26.106150Z"
    },
    "papermill": {
     "duration": 0.015611,
     "end_time": "2021-10-26T04:20:30.861553",
     "exception": false,
     "start_time": "2021-10-26T04:20:30.845942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def searchNearest(prediction):\n",
    "    idx = np.searchsorted(df_data_y_sort, prediction)\n",
    "    #print(\"prediction= %s, idx= %s\" %(prediction,  idx))\n",
    "    if (idx >= df_data_y_sort.shape[0]-1):\n",
    "        ## If the number is greater than the largest value in sort array, return the largest element\n",
    "        return df_data_y_sort[-1]\n",
    "    \n",
    "    lowerVal = df_data_y_sort[idx]\n",
    "    upperVal = df_data_y_sort[idx+1]\n",
    "    return lowerVal if np.abs(lowerVal - prediction) < np.abs(upperVal - prediction) else upperVal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd228114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T04:20:30.891249Z",
     "iopub.status.busy": "2021-10-26T04:20:30.890408Z",
     "iopub.status.idle": "2021-10-26T04:26:26.770866Z",
     "shell.execute_reply": "2021-10-26T04:26:26.771477Z",
     "shell.execute_reply.started": "2021-10-26T03:59:34.044890Z"
    },
    "papermill": {
     "duration": 355.901294,
     "end_time": "2021-10-26T04:26:26.771656",
     "exception": false,
     "start_time": "2021-10-26T04:20:30.870362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6036000, 73)\n",
      "Mem. usage decreased from 2757.31 Mb to 1093.71 Mb (60.3% reduction)\n",
      "[-9.8905218e-01 -9.3738443e-01 -1.0000000e+00 -8.6412060e-01\n",
      " -8.4252667e-01 -8.1925792e-01 -7.9415160e-01 -7.6915002e-01\n",
      " -7.3844558e-01  2.8902071e+00  3.6640253e+00  3.7439151e+00\n",
      "  4.2778239e+00  4.6832628e+00  4.6862230e+00 -7.2522783e-01\n",
      " -5.1658052e-01  4.7140878e-01  2.1799320e-01  1.2102799e-01\n",
      "  6.8314672e-02  2.3523314e-02  3.1109562e-03 -1.0000000e+00\n",
      " -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00\n",
      " -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00\n",
      " -1.0000000e+00 -1.0000000e+00 -1.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  3.2694092e-01  1.2738636e+00  0.0000000e+00\n",
      " -6.6554129e-01 -2.8571430e-01  1.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  1.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -4.9665764e-01 -6.7441475e-01 -6.0515839e-01\n",
      " -1.0000000e+00 -8.0625820e-01  7.8934114e-03 -4.2843562e-01\n",
      " -6.6874975e-01 -3.6459795e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(75450, 80, 70)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Added features for data and test\n",
    "df_data = addFeatures(df_data)\n",
    "#df_test = addFeatures(df_test)\n",
    "print(df_data.shape)\n",
    "#print(df_test.shape)\n",
    "\n",
    "##Reduce memory data occupied by changing the datatypes\n",
    "df_data = reduce_mem_usage(df_data)\n",
    "#print(df_data.dtypes)\n",
    "#df_test = reduce_mem_usage(df_test)\n",
    "#df_test.dtypes\n",
    "\n",
    "df_y_data = df_data[\"pressure\"].copy()\n",
    "df_data_y_sort = np.sort(df_y_data.unique())\n",
    "\n",
    "targets = df_y_data.to_numpy().reshape(-1, 80)\n",
    "df_data = df_data.drop([\"id\", \"breath_id\", \"pressure\"], axis=1)\n",
    "\n",
    "#df_test = df_test.drop([\"id\", \"breath_id\"], axis=1)\n",
    "#print(\"data shape= %s, Test shape= %s\" %(df_data.shape, df_test.shape))\n",
    "\n",
    "##Normalize the data\n",
    "from sklearn.preprocessing import RobustScaler, normalize\n",
    "RS = RobustScaler()\n",
    "train = RS.fit_transform(df_data)\n",
    "#test = RS.transform(df_test)\n",
    "print(train[0,:])\n",
    "#print(test[0,:])\n",
    "\n",
    "##Change shape for RNN runs\n",
    "train = train.reshape(-1, 80, train.shape[-1])\n",
    "#test = test.reshape(-1, 80, train.shape[-1])\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3014ab23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T04:26:26.795792Z",
     "iopub.status.busy": "2021-10-26T04:26:26.795184Z",
     "iopub.status.idle": "2021-10-26T04:26:37.401079Z",
     "shell.execute_reply": "2021-10-26T04:26:37.400568Z",
     "shell.execute_reply.started": "2021-10-26T03:59:49.150146Z"
    },
    "papermill": {
     "duration": 10.619688,
     "end_time": "2021-10-26T04:26:37.401225",
     "exception": false,
     "start_time": "2021-10-26T04:26:26.781537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 04:26:26.904009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-26 04:26:26.995344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-26 04:26:26.996086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-26 04:26:26.997639: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-26 04:26:26.998777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-26 04:26:26.999501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-26 04:26:27.000172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-26 04:26:28.700357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-26 04:26:28.701187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-26 04:26:28.701856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-26 04:26:28.703219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 80, 2048)          8970240   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 80, 1024)          10489856  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 80, 512)           2623488   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 80, 256)           656384    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 80, 128)           32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80, 1)             129       \n",
      "=================================================================\n",
      "Total params: 22,772,993\n",
      "Trainable params: 22,772,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##Load model\n",
    "##Clear models\n",
    "tf.keras.backend.clear_session()\n",
    "model = load_model(\"/kaggle/input/rnn-64-128/ventilator_extraFeatures_biDirectional1_trial.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee95c65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T04:26:37.427831Z",
     "iopub.status.busy": "2021-10-26T04:26:37.426157Z",
     "iopub.status.idle": "2021-10-26T04:26:37.428468Z",
     "shell.execute_reply": "2021-10-26T04:26:37.428895Z",
     "shell.execute_reply.started": "2021-10-26T04:15:47.594893Z"
    },
    "papermill": {
     "duration": 0.017031,
     "end_time": "2021-10-26T04:26:37.429030",
     "exception": false,
     "start_time": "2021-10-26T04:26:37.411999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(train.shape[2])\n",
    "# print(type(train))\n",
    "# new_train = train.reshape(-1, int(train.shape[2]))\n",
    "# print(train[0][0])\n",
    "# print(new_train[0])\n",
    "\n",
    "# pred = model.predict(train, batch_size=32)\n",
    "# pred = pred.reshape(-1, int(pred.shape[2]))\n",
    "# type(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "144488b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T04:26:37.459312Z",
     "iopub.status.busy": "2021-10-26T04:26:37.457760Z",
     "iopub.status.idle": "2021-10-26T04:31:57.468097Z",
     "shell.execute_reply": "2021-10-26T04:31:57.468871Z"
    },
    "papermill": {
     "duration": 320.029812,
     "end_time": "2021-10-26T04:31:57.469277",
     "exception": true,
     "start_time": "2021-10-26T04:26:37.439465",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 04:26:38.261796: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1690080000 exceeds 10% of free system memory.\n",
      "2021-10-26 04:26:39.951246: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1690080000 exceeds 10% of free system memory.\n",
      "2021-10-26 04:26:41.173082: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-10-26 04:26:43.872564: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(75450, 80, 70)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25/1776906881.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m##Concat even the additional features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msaveName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_trainErrorAnalysis_allFeatures.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                     \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m                     \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m                 )\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(75450, 80, 70)"
     ]
    }
   ],
   "source": [
    "##Error analysis files\n",
    "##Predict the train pressures\n",
    "df_y_data = df_y_data.to_frame()\n",
    "trainPred = model.predict(train, batch_size=32)\n",
    "df_y_data[\"pressPredict\"] = trainPred.reshape(-1, int(trainPred.shape[2]))\n",
    "df_y_data[\"nearestPredict\"] = df_y_data[\"pressPredict\"].apply(searchNearest)\n",
    "\n",
    "##Load original data\n",
    "trainDataPath = \"/kaggle/input/ventilator-pressure-prediction/train.csv\"\n",
    "df_data = pd.read_csv(trainDataPath, header=0)\n",
    "\n",
    "df_data.drop(\"pressure\", axis=1, inplace=True) ##Drop pressure, since df_y_data has pressure\n",
    "\n",
    "##SaveName\n",
    "saveName = \"ventilator_extraFeatures_biDirectional1_trial\"\n",
    "\n",
    "##Concat data\n",
    "df_data = pd.concat([df_data, df_y_data], axis=1, ignore_index=True)\n",
    "df_data.to_csv(\"./\" + saveName + \"_trainErrorAnalysis.csv\", index=False)\n",
    "\n",
    "##Concat even the additional features\n",
    "df_data = pd.concat([df_data, pd.DataFrame(data=train)], axis=1, ignore_index=True)\n",
    "df_data.to_csv(\"./\" + saveName + \"_trainErrorAnalysis_allFeatures.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb123f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T19:13:23.290923Z",
     "iopub.status.busy": "2021-10-25T19:13:23.290651Z",
     "iopub.status.idle": "2021-10-25T19:13:23.581693Z",
     "shell.execute_reply": "2021-10-25T19:13:23.580809Z",
     "shell.execute_reply.started": "2021-10-25T19:13:23.290895Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ##Compare actual pressure and predicted pressures\n",
    "# ##Visualize some data\n",
    "# import matplotlib.pyplot as plt\n",
    "# rowsToViz = 80*5\n",
    "# vizData = df_data.iloc[:rowsToViz,:]\n",
    "# pressData = df_y_data.iloc[:rowsToViz, :]\n",
    "# vizData.head()\n",
    "\n",
    "# plt.figure(figsize=(30, 6))\n",
    "# plt.plot(vizData[\"R_div_C\"], label='R_div_C')\n",
    "# #plt.plot(vizData[\"C\"], label=\"C\")\n",
    "# plt.plot(vizData[\"u_in\"], label=\"u_in\")\n",
    "# plt.plot(vizData[\"u_out\"], label=\"u_out\")\n",
    "# plt.plot(pressData[\"pressure\"], label=\"pressure\")\n",
    "# plt.plot(pressData[\"pressPredict\"], label=\"pressPredict\")\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 709.613844,
   "end_time": "2021-10-26T04:32:00.267395",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-26T04:20:10.653551",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
